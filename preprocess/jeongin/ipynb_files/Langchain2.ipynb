{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BERTDataset' from 'dataset' (/opt/conda/lib/python3.10/site-packages/dataset/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMChain\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BERTDataset\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'BERTDataset' from 'dataset' (/opt/conda/lib/python3.10/site-packages/dataset/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "import evaluate\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "from langchain.llms import OpenAI, HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from dataset import BERTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 로드 및 설정\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "print(\"데이터셋 로드 및 설정\")\n",
    "# ================================================\n",
    "\n",
    "# 랜덤 시드 설정\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model_name = 'klue/bert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "df_train = pd.read_csv(\"/data/ephemeral/home/data/train.csv\")\n",
    "df_valid = pd.read_csv(\"/data/ephemeral/home/data/valid_output.csv\")\n",
    "\n",
    "\n",
    "# 데이터셋에 인덱스 컬럼 추가\n",
    "df_train = df_train.reset_index().rename(columns={'index': 'idx'})\n",
    "df_valid = df_valid.reset_index().rename(columns={'index': 'idx'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pc = df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "JVMNotFoundException",
     "evalue": "No JVM shared library file (libjvm.so) found. Try setting up the JAVA_HOME environment variable properly.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJVMNotFoundException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^ ㄱ-ㅣ가-힣]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#형태소 분석 및 불용어 제거 함수\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m okt \u001b[38;5;241m=\u001b[39m \u001b[43mOkt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m korean_stopwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m은\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m는\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m이\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m가\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m을\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m를\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m의\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m에\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m에서\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m로\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m으로\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m과\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m와\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m도\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m만\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m에게\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m께\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m한테\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m보다\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m라고\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m이라고\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m으로서\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m같이\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m처럼\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m만큼\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_and_remove_stopwords\u001b[39m(text):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/konlpy/tag/_okt.py:51\u001b[0m, in \u001b[0;36mOkt.__init__\u001b[0;34m(self, jvmpath, max_heap_size)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, jvmpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_heap_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jpype\u001b[38;5;241m.\u001b[39misJVMStarted():\n\u001b[0;32m---> 51\u001b[0m         \u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_jvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjvmpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_heap_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     oktJavaPackage \u001b[38;5;241m=\u001b[39m jpype\u001b[38;5;241m.\u001b[39mJPackage(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkr.lucypark.okt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m     OktInterfaceJavaClass \u001b[38;5;241m=\u001b[39m oktJavaPackage\u001b[38;5;241m.\u001b[39mOktInterface\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/konlpy/jvm.py:55\u001b[0m, in \u001b[0;36minit_jvm\u001b[0;34m(jvmpath, max_heap_size)\u001b[0m\n\u001b[1;32m     52\u001b[0m args \u001b[38;5;241m=\u001b[39m [javadir, os\u001b[38;5;241m.\u001b[39msep]\n\u001b[1;32m     53\u001b[0m classpath \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m folder_suffix]\n\u001b[0;32m---> 55\u001b[0m jvmpath \u001b[38;5;241m=\u001b[39m jvmpath \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mjpype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetDefaultJVMPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# NOTE: Temporary patch for Issue #76. Erase when possible.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m'\u001b[39m\\\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m jvmpath\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.8.0\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\\\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m jvmpath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlibjvm.dylib\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jpype/_jvmfinder.py:74\u001b[0m, in \u001b[0;36mgetDefaultJVMPath\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     finder \u001b[38;5;241m=\u001b[39m LinuxJVMFinder()\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfinder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_jvm_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jpype/_jvmfinder.py:212\u001b[0m, in \u001b[0;36mJVMFinder.get_jvm_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jvm_notsupport_ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jvm_notsupport_ext\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m JVMNotFoundException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo JVM shared library file (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    213\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound. Try setting up the JAVA_HOME \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    214\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment variable properly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m                            \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libfile))\n",
      "\u001b[0;31mJVMNotFoundException\u001b[0m: No JVM shared library file (libjvm.so) found. Try setting up the JAVA_HOME environment variable properly."
     ]
    }
   ],
   "source": [
    "#특수 기호 제거 함수:\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^ ㄱ-ㅣ가-힣]', '', text)\n",
    "\n",
    "\n",
    "#형태소 분석 및 불용어 제거 함수\n",
    "okt = Okt()\n",
    "korean_stopwords = set(['은', '는', '이', '가', '을', '를', '의', '에', '에서', '로', '으로', '과', '와', '도', '만', '에게', '께', '한테', '보다', '라고', '이라고', '으로서', '같이', '처럼', '만큼'])\n",
    "\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    tokens = okt.pos(text, stem=True)\n",
    "    return ' '.join([word for word, pos in tokens if pos in ['Noun', 'Verb', 'Adjective'] and word not in korean_stopwords])\n",
    "\n",
    "#2글자 이상 단어 필터링 함수\n",
    "def filter_tokens_by_length(text, min_length=2):\n",
    "    return ' '.join([word for word in text.split() if len(word) >= min_length])\n",
    "\n",
    "\n",
    "#전체 전처리 함수\n",
    "def preprocess_text(text):\n",
    "    text = remove_special_characters(text)\n",
    "    text = tokenize_and_remove_stopwords(text)\n",
    "    text = filter_tokens_by_length(text)  # 2글자 이상 단어 필터링 추가\n",
    "    return text\n",
    "\n",
    "\n",
    "#DataFrame에 전처리 적용\n",
    "def preprocess_dataframe(df):\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "    return df\n",
    "\n",
    "\n",
    "# 데이터프레임 전처리\n",
    "df_cleaned = preprocess_dataframe(df_train_pc)\n",
    "\n",
    "# 정제된 문서 리스트 반환\n",
    "cleaned_texts = df_cleaned['cleaned_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-huggingface in /opt/conda/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langchain-huggingface) (0.26.2)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from langchain-huggingface) (0.3.15)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from langchain-huggingface) (3.2.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from langchain-huggingface) (0.20.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in /opt/conda/lib/python3.10/site-packages (from langchain-huggingface) (4.46.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2023.9.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.1.139)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (9.0.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.5.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.14.1)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (9.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.0->langchain-huggingface) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.0->langchain-huggingface) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2024.8.30)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain-huggingface) (1.0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정제된 문서 리스트 반환\n",
    "texts = df_cleaned['cleaned_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4077282/1460730533.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"nlpai-lab/KoE5\")\n"
     ]
    }
   ],
   "source": [
    "#문자 임베딩\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"nlpai-lab/KoE5\")\n",
    "text_embeddings = embeddings.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=7, random_state=42)\n",
    "\n",
    "\n",
    "cluster_labels = kmeans.fit_predict(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 5, ..., 1, 6, 5], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_text(cluster_center, embeddings, texts):\n",
    "    distances = np.linalg.norm(embeddings - cluster_center, axis=1)\n",
    "    closest_index = np.argmin(distances)\n",
    "    return texts[closest_index]\n",
    "\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "representative_texts = [find_closest_text(center, text_embeddings, texts) for center in cluster_centers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf8aa2a3a144db4aab9d2790e287916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"beomi/llama-2-ko-7b\"  # 또는 다른 한국어 Llama 모델\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=50,  # 출력 길이를 크게 줄임\n",
    "    do_sample=True,\n",
    "    temperature=0.5,  # 낮은 temperature로 더 결정적인 출력\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"뉴스 기사의 일부 단어입니다:\n",
    "\n",
    "{text}\n",
    "\n",
    "위 단어들과 가장 관련 있는 뉴스 도메인을 한 단어로만 답하세요. 예시: 정치, 경제, 사회, 문화, 국제, 과학, 스포츠.\n",
    "다른 설명이나 추가 정보 없이 오직 도메인 단어 하나만 답하세요.\n",
    "\n",
    "도메인:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: 뉴스 기사의 일부 단어입니다:\n",
      "\n",
      "언론 만남\n",
      "\n",
      "위 단어들과 가장 관련 있는 뉴스 도메인을 한 단어로만 답하세요. 예시: 정치, 경제, 사회, 문화, 국제, 과학, 스포츠.\n",
      "다른 설명이나 추가 정보 없이 오직 도메인 단어 하나만 답하세요.\n",
      "\n",
      "도메인:\n",
      "Raw response: 뉴스 기사의 일부 단어입니다:\n",
      "\n",
      "언론 만남\n",
      "\n",
      "위 단어들과 가장 관련 있는 뉴스 도메인을 한 단어로만 답하세요. 예시: 정치, 경제, 사회, 문화, 국제, 과학, 스포츠.\n",
      "다른 설명이나 추가 정보 없이 오직 도메인 단어 하나만 답하세요.\n",
      "\n",
      "도메인:​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​\n",
      "Extracted domain: Unknown\n",
      "Input text: 뉴스 기사의 일부 단어입니다:\n",
      "\n",
      "출시 출리 오르다\n",
      "\n",
      "위 단어들과 가장 관련 있는 뉴스 도메인을 한 단어로만 답하세요. 예시: 정치, 경제, 사회, 문화, 국제, 과학, 스포츠.\n",
      "다른 설명이나 추가 정보 없이 오직 도메인 단어 하나만 답하세요.\n",
      "\n",
      "도메인:\n",
      "Raw response: 뉴스 기사의 일부 단어입니다:\n",
      "\n",
      "출시 출리 오르다\n",
      "\n",
      "위 단어들과 가장 관련 있는 뉴스 도메인을 한 단어로만 답하세요. 예시: 정치, 경제, 사회, 문화, 국제, 과학, 스포츠.\n",
      "다른 설명이나 추가 정보 없이 오직 도메인 단어 하나만 답하세요.\n",
      "\n",
      "도메인:​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​\n",
      "Extracted domain: Unknown\n",
      "Input text: 뉴스 기사의 일부 단어입니다:\n",
      "\n",
      "서로\n",
      "\n",
      "위 단어들과 가장 관련 있는 뉴스 도메인을 한 단어로만 답하세요. 예시: 정치, 경제, 사회, 문화, 국제, 과학, 스포츠.\n",
      "다른 설명이나 추가 정보 없이 오직 도메인 단어 하나만 답하세요.\n",
      "\n",
      "도메인:\n",
      "Raw response: 뉴스 기사의 일부 단어입니다:\n",
      "\n",
      "서로\n",
      "\n",
      "위 단어들과 가장 관련 있는 뉴스 도메인을 한 단어로만 답하세요. 예시: 정치, 경제, 사회, 문화, 국제, 과학, 스포츠.\n",
      "다른 설명이나 추가 정보 없이 오직 도메인 단어 하나만 답하세요.\n",
      "\n",
      "도메인:​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​\n",
      "Extracted domain: Unknown\n",
      "Input text: 뉴스 기사의 일부 단어입니다:\n",
      "\n",
      "이란 상대 국적 체포 갈등 구금 학대 서로 비방\n",
      "\n",
      "위 단어들과 가장 관련 있는 뉴스 도메인을 한 단어로만 답하세요. 예시: 정치, 경제, 사회, 문화, 국제, 과학, 스포츠.\n",
      "다른 설명이나 추가 정보 없이 오직 도메인 단어 하나만 답하세요.\n",
      "\n",
      "도메인:\n",
      "Raw response: 뉴스 기사의 일부 단어입니다:\n",
      "\n",
      "이란 상대 국적 체포 갈등 구금 학대 서로 비방\n",
      "\n",
      "위 단어들과 가장 관련 있는 뉴스 도메인을 한 단어로만 답하세요. 예시: 정치, 경제, 사회, 문화, 국제, 과학, 스포츠.\n",
      "다른 설명이나 추가 정보 없이 오직 도메인 단어 하나만 답하세요.\n",
      "\n",
      "도메인:​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​\n",
      "Extracted domain: Unknown\n",
      "Input text: 뉴스 기사의 일부 단어입니다:\n",
      "\n",
      "전국 아침 기온 최대 주말\n",
      "\n",
      "위 단어들과 가장 관련 있는 뉴스 도메인을 한 단어로만 답하세요. 예시: 정치, 경제, 사회, 문화, 국제, 과학, 스포츠.\n",
      "다른 설명이나 추가 정보 없이 오직 도메인 단어 하나만 답하세요.\n",
      "\n",
      "도메인:\n",
      "Raw response: 뉴스 기사의 일부 단어입니다:\n",
      "\n",
      "전국 아침 기온 최대 주말\n",
      "\n",
      "위 단어들과 가장 관련 있는 뉴스 도메인을 한 단어로만 답하세요. 예시: 정치, 경제, 사회, 문화, 국제, 과학, 스포츠.\n",
      "다른 설명이나 추가 정보 없이 오직 도메인 단어 하나만 답하세요.\n",
      "\n",
      "도메인: ​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​\n",
      "Extracted domain: Unknown\n",
      "Input text: 뉴스 기사의 일부 단어입니다:\n",
      "\n",
      "정상 회의 심각하다 목소리 내다\n",
      "\n",
      "위 단어들과 가장 관련 있는 뉴스 도메인을 한 단어로만 답하세요. 예시: 정치, 경제, 사회, 문화, 국제, 과학, 스포츠.\n",
      "다른 설명이나 추가 정보 없이 오직 도메인 단어 하나만 답하세요.\n",
      "\n",
      "도메인:\n",
      "Raw response: 뉴스 기사의 일부 단어입니다:\n",
      "\n",
      "정상 회의 심각하다 목소리 내다\n",
      "\n",
      "위 단어들과 가장 관련 있는 뉴스 도메인을 한 단어로만 답하세요. 예시: 정치, 경제, 사회, 문화, 국제, 과학, 스포츠.\n",
      "다른 설명이나 추가 정보 없이 오직 도메인 단어 하나만 답하세요.\n",
      "\n",
      "도메인:​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​\n",
      "Extracted domain: Unknown\n",
      "Input text: 뉴스 기사의 일부 단어입니다:\n",
      "\n",
      "기다 이닝 물다 점프\n",
      "\n",
      "위 단어들과 가장 관련 있는 뉴스 도메인을 한 단어로만 답하세요. 예시: 정치, 경제, 사회, 문화, 국제, 과학, 스포츠.\n",
      "다른 설명이나 추가 정보 없이 오직 도메인 단어 하나만 답하세요.\n",
      "\n",
      "도메인:\n",
      "Raw response: 뉴스 기사의 일부 단어입니다:\n",
      "\n",
      "기다 이닝 물다 점프\n",
      "\n",
      "위 단어들과 가장 관련 있는 뉴스 도메인을 한 단어로만 답하세요. 예시: 정치, 경제, 사회, 문화, 국제, 과학, 스포츠.\n",
      "다른 설명이나 추가 정보 없이 오직 도메인 단어 하나만 답하세요.\n",
      "\n",
      "도메인:​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​\n",
      "Extracted domain: Unknown\n",
      "Final domains: ['Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown']\n"
     ]
    }
   ],
   "source": [
    "domains = []\n",
    "for text in representative_texts:\n",
    "    input_text = prompt.format(text=text)\n",
    "    print(f\"Input text: {input_text}\")  # 입력 텍스트 확인\n",
    "    \n",
    "    response = llm.invoke(input_text)\n",
    "    print(f\"Raw response: {response}\")  # 원본 응답 확인\n",
    "    \n",
    "    # 응답 처리\n",
    "    if isinstance(response, str):\n",
    "        # 응답에서 '도메인:' 이후의 텍스트만 추출\n",
    "        domain_parts = response.split('도메인:')\n",
    "        if len(domain_parts) > 1:\n",
    "            domain = domain_parts[-1].strip().split()[0]  # 첫 단어만 추출\n",
    "        else:\n",
    "            domain = \"Unknown\"\n",
    "    else:\n",
    "        domain = \"Unknown\"\n",
    "    \n",
    "    # 도메인 필터링\n",
    "    if len(domain) > 15 or '://' in domain or domain.lower() in ['뉴스', '기사', '단어']:\n",
    "        domain = \"Unknown\"\n",
    "    \n",
    "    print(f\"Extracted domain: {domain}\")  # 추출된 도메인 확인\n",
    "    domains.append(domain)\n",
    "\n",
    "print(\"Final domains:\", domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final domains: ['Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown', 'Unknown']\n"
     ]
    }
   ],
   "source": [
    "print(\"Final domains:\", domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beomi/llama-2-ko-7b\"  \n",
    "# ['다음은', '다음은', '다음은', '다음은', '다음은', '다음은', '다음은']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#서버 필요없는 토큰 모델 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del tokenizer\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n미사 이용기 종보 토큰화 해서 \\n\"미사\"는 \"이용기\"와 \"종보\"와의 관계를 고려하여 표현됩니다.\\n\"이용기\"는 \"미사\"와 \"종보\"와의 관계를 고려합니다.\\n\"종보\"도 마찬가지로 다른 단어들과의 관계를 고려합니다.\\n\\n의문1)\\n\\n그렇다면 미사 이용기 종보는 뜻을 가지고 있지 않는 단어로 인식하지 않을 수 있음 그렇다면 단어가 손상 되었어도 임베딩 할때 중요도를 깨달을 수 있는 모델을 사용하는 것이 좋지 않을까?\\n\\n서브워드 토큰화 (Subword Tokenization):\\nFastText와 같은 모델은 단어를 더 작은 단위(서브워드)로 분해하여 처리합니다3. 이 방식은 오타나 변형된 단어도 어느 정도 처리할 수 있게 해줍니다.\\n문자 수준 임베딩 (Character-level Embeddings):\\n문자 단위로 임베딩을 생성하는 방식입니다5. 이는 알 수 없는 단어나 오타에 대해서도 어느 정도의 의미를 추출할 수 있게 해줍니다.\\n바이트 페어 인코딩 (Byte Pair Encoding, BPE):\\nBPE는 자주 등장하는 문자 시퀀스를 하나의 토큰으로 취급합니다5. 이 방식은 알 수 없는 단어나 오타에 대해서도 유연하게 대응할 수 있습니다.\\n컨텍스트 기반 임베딩과 문자 수준 임베딩의 결합:\\nBERT나 ELMo와 같은 컨텍스트 기반 모델과 문자 수준 임베딩을 결합하는 방식을 고려할 수 있습니다13. 이렇게 하면 단어의 컨텍스트와 문자 수준의 정보를 모두 활용할 수 있습니다.\\n사전 처리 및 정규화:\\n텍스트 데이터에 대한 철저한 사전 처리와 정규화 과정을 거치면, 손상된 단어나 오타를 어느 정도 정정할 수 있습니다.\\n도메인 특화 임베딩:\\n특정 도메인에 특화된 임베딩 모델을 사용하거나 fine-tuning하는 것도 좋은 방법입니다3. 이는 해당 도메인에서 자주 등장하는 특수한 단어나 표현을 더 잘 처리할 수 있게 해줍니다.\\n이러한 접근 방식들은 단어가 손상되었거나 의미를 가지고 있지 않은 것처럼 보이는 경우에도 더 robust한 임베딩을 생성할 수 있게 해줍니다. 특히 서브워드 토큰화나 문자 수준 임베딩을 활용하면, \\n\"미사\", \"이용기\", \"종보\"와 같은 단어들이 완전히 의미 없는 것으로 취급되지 않고, 그 구성 요소나 문자 수준에서의 패턴을 통해 어느 정도의 의미를 추출할 수 있습니다.\\n\\n\\n이러한 접근 방식들은 단어가 손상되었거나 의미를 가지고 있지 않은 것처럼 보이는 경우에도 더 robust한 임베딩을 생성할 수 있게 해줍니다. 특히 서브워드 토큰화나 문자 수준 임베딩을 활용하면, \"미사\", \"이용기\", \"종보\"와 같은 단어들이 완전히 의미 없는 것으로 취급되지 않고, 그 구성 요소나 문자 수준에서의 패턴을 통해 어느 정도의 의미를 추출할 수 있습니다.\\n결론적으로, 귀하의 제안처럼 단어가 손상되었거나 의미를 파악하기 어려운 경우에도 중요도를 인식할 수 있는 모델을 사용하는 것이 더 효과적일 수 있습니다. 이는 특히 실제 응용에서 노이즈가 있는 데이터나 도메인 특화된 용어를 다룰 때 매우 중요한 고려사항이 될 수 있습니다.\\n\\n\\n의문 2)\\n형태소로 구분을 할때 의미가 없는 단어들은 제거하는게 맞지 않을까?\\n\\n주의할 점\\n도메인 특수성: 특정 도메인에서는 일반적인 불용어가 중요한 의미를 가질 수 있습니다.\\n문맥 손실: 과도한 단어 제거는 문장의 전체적인 문맥을 왜곡시킬 수 있습니다.\\n부정어 처리: \\'않다\\', \\'아니다\\' 등의 부정어는 문장의 의미를 완전히 바꿀 수 있으므로 주의가 필요합니다.\\n대부분의 NLP 작업에서 효과적일 것입니다. 다만, 특정 작업이나 도메인에 따라 이 전처리 과정을 약간 수정하거나 조정할 필요가 있을 수 있습니다.\\n\\n\\n \\'날씨 새해 첫날 전국 가끔 구름 많다 아침 기온 영하\\',\\n \\'개다 급전 패키지 나다 한정 판매\\',\\n \\'애위 최고 스마트\\',\\n \\'최대 트밤 의전\\',\\n \\'고속 운행 차다\\',\\n \\'스토어 유통 수수료 인하\\',\\n \\'블블 출시\\',\\n \\'대다 영정 담다 하다 종합\\',\\n \\'사회 서비스 사무장 직원\\',\\n \\'트럼프 주장 불법이민 전원 추방 미국 경제 축소\\',\\n \\'불교 육식 금지 돼다\\',\\n \\'최고 위원 출마 위해\\',\\n \\'민주 평창올림픽 성공 개최 총력전 색깔론 한국 맹공\\',\\n \\'창녕 육박 불볕 더위 물놀이 여름 축제 더위 탈출 종합\\',\\n \\'해운 위기 용료 인하 합병 해결 되다\\',\\n \\'대판 파라오 엘시 이집트 마지막 독립 언론 겁박\\',\\n \\'이주열 통화정책 완화 정도 추가 조정 필요\\',\\n \\'대다 드이 윈도 점율 최초\\',\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nlpai-lab/KoE5 모델은 트랜스포머 아키텍처를 기반으로 하며, 이는 자기주의(self-attention) 메커니즘을 사용하여 문장의 각 부분이 서로 어떻게 관련되어 있는지를 파악합니다\n",
    "'''\n",
    "미사 이용기 종보 토큰화 해서 \n",
    "\"미사\"는 \"이용기\"와 \"종보\"와의 관계를 고려하여 표현됩니다.\n",
    "\"이용기\"는 \"미사\"와 \"종보\"와의 관계를 고려합니다.\n",
    "\"종보\"도 마찬가지로 다른 단어들과의 관계를 고려합니다.\n",
    "\n",
    "의문1)\n",
    "\n",
    "그렇다면 미사 이용기 종보는 뜻을 가지고 있지 않는 단어로 인식하지 않을 수 있음 그렇다면 단어가 손상 되었어도 임베딩 할때 중요도를 깨달을 수 있는 모델을 사용하는 것이 좋지 않을까?\n",
    "\n",
    "서브워드 토큰화 (Subword Tokenization):\n",
    "FastText와 같은 모델은 단어를 더 작은 단위(서브워드)로 분해하여 처리합니다3. 이 방식은 오타나 변형된 단어도 어느 정도 처리할 수 있게 해줍니다.\n",
    "문자 수준 임베딩 (Character-level Embeddings):\n",
    "문자 단위로 임베딩을 생성하는 방식입니다5. 이는 알 수 없는 단어나 오타에 대해서도 어느 정도의 의미를 추출할 수 있게 해줍니다.\n",
    "바이트 페어 인코딩 (Byte Pair Encoding, BPE):\n",
    "BPE는 자주 등장하는 문자 시퀀스를 하나의 토큰으로 취급합니다5. 이 방식은 알 수 없는 단어나 오타에 대해서도 유연하게 대응할 수 있습니다.\n",
    "컨텍스트 기반 임베딩과 문자 수준 임베딩의 결합:\n",
    "BERT나 ELMo와 같은 컨텍스트 기반 모델과 문자 수준 임베딩을 결합하는 방식을 고려할 수 있습니다13. 이렇게 하면 단어의 컨텍스트와 문자 수준의 정보를 모두 활용할 수 있습니다.\n",
    "사전 처리 및 정규화:\n",
    "텍스트 데이터에 대한 철저한 사전 처리와 정규화 과정을 거치면, 손상된 단어나 오타를 어느 정도 정정할 수 있습니다.\n",
    "도메인 특화 임베딩:\n",
    "특정 도메인에 특화된 임베딩 모델을 사용하거나 fine-tuning하는 것도 좋은 방법입니다3. 이는 해당 도메인에서 자주 등장하는 특수한 단어나 표현을 더 잘 처리할 수 있게 해줍니다.\n",
    "이러한 접근 방식들은 단어가 손상되었거나 의미를 가지고 있지 않은 것처럼 보이는 경우에도 더 robust한 임베딩을 생성할 수 있게 해줍니다. 특히 서브워드 토큰화나 문자 수준 임베딩을 활용하면, \n",
    "\"미사\", \"이용기\", \"종보\"와 같은 단어들이 완전히 의미 없는 것으로 취급되지 않고, 그 구성 요소나 문자 수준에서의 패턴을 통해 어느 정도의 의미를 추출할 수 있습니다.\n",
    "\n",
    "\n",
    "이러한 접근 방식들은 단어가 손상되었거나 의미를 가지고 있지 않은 것처럼 보이는 경우에도 더 robust한 임베딩을 생성할 수 있게 해줍니다. 특히 서브워드 토큰화나 문자 수준 임베딩을 활용하면, \"미사\", \"이용기\", \"종보\"와 같은 단어들이 완전히 의미 없는 것으로 취급되지 않고, 그 구성 요소나 문자 수준에서의 패턴을 통해 어느 정도의 의미를 추출할 수 있습니다.\n",
    "결론적으로, 귀하의 제안처럼 단어가 손상되었거나 의미를 파악하기 어려운 경우에도 중요도를 인식할 수 있는 모델을 사용하는 것이 더 효과적일 수 있습니다. 이는 특히 실제 응용에서 노이즈가 있는 데이터나 도메인 특화된 용어를 다룰 때 매우 중요한 고려사항이 될 수 있습니다.\n",
    "\n",
    "\n",
    "의문 2)\n",
    "형태소로 구분을 할때 의미가 없는 단어들은 제거하는게 맞지 않을까?\n",
    "\n",
    "주의할 점\n",
    "도메인 특수성: 특정 도메인에서는 일반적인 불용어가 중요한 의미를 가질 수 있습니다.\n",
    "문맥 손실: 과도한 단어 제거는 문장의 전체적인 문맥을 왜곡시킬 수 있습니다.\n",
    "부정어 처리: '않다', '아니다' 등의 부정어는 문장의 의미를 완전히 바꿀 수 있으므로 주의가 필요합니다.\n",
    "대부분의 NLP 작업에서 효과적일 것입니다. 다만, 특정 작업이나 도메인에 따라 이 전처리 과정을 약간 수정하거나 조정할 필요가 있을 수 있습니다.\n",
    "\n",
    "\n",
    " '날씨 새해 첫날 전국 가끔 구름 많다 아침 기온 영하',\n",
    " '개다 급전 패키지 나다 한정 판매',\n",
    " '애위 최고 스마트',\n",
    " '최대 트밤 의전',\n",
    " '고속 운행 차다',\n",
    " '스토어 유통 수수료 인하',\n",
    " '블블 출시',\n",
    " '대다 영정 담다 하다 종합',\n",
    " '사회 서비스 사무장 직원',\n",
    " '트럼프 주장 불법이민 전원 추방 미국 경제 축소',\n",
    " '불교 육식 금지 돼다',\n",
    " '최고 위원 출마 위해',\n",
    " '민주 평창올림픽 성공 개최 총력전 색깔론 한국 맹공',\n",
    " '창녕 육박 불볕 더위 물놀이 여름 축제 더위 탈출 종합',\n",
    " '해운 위기 용료 인하 합병 해결 되다',\n",
    " '대판 파라오 엘시 이집트 마지막 독립 언론 겁박',\n",
    " '이주열 통화정책 완화 정도 추가 조정 필요',\n",
    " '대다 드이 윈도 점율 최초',\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
