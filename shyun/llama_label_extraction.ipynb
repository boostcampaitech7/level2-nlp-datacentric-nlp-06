{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import hanja # !pip install hanja\n",
    "import pandas as pd\n",
    "\n",
    "train_dataset = pd.read_csv('../../../data/train.csv')\n",
    "train_texts = list(train_dataset['text'].values)\n",
    "IDs = list(train_dataset['ID'].values)\n",
    "\n",
    "korean = re.compile('[^가-힣...…\\s]+') # 한국어, ..., …, 공백 외의 문자가 한 번 이상 등장할 경우에 대한 패턴\n",
    "\n",
    "train_dataset['polluted_lv'] = None\n",
    "train_dataset['no_hanja'] = None\n",
    "for id_ in IDs:\n",
    "    text = train_dataset[train_dataset['ID']==id_]['text'].values[0]\n",
    "    text = hanja.translate(text, 'substitution')\n",
    "    text = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", text)\n",
    "    \n",
    "    results = korean.findall(text)\n",
    "    total = sum([len(r) for r in results])\n",
    "    prob = total / len(text)\n",
    "    train_dataset.loc[train_dataset['ID']==id_, 'polluted_lv'] = prob\n",
    "    train_dataset.loc[train_dataset['ID']==id_, 'no_hanja'] = text\n",
    "\n",
    "# train_dataset.to_csv('train-no_special-no_hanja-polluted_lv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>polluted_lv</th>\n",
       "      <th>no_hanja</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>ynat-v1_train_00639</td>\n",
       "      <td>中o선제적(입막음…2S·IBM|* 사이46안8 논의에 끌어들여</td>\n",
       "      <td>6</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>중o선제적입막음2SIBM 사이46안8 논의에 끌어들여</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>ynat-v1_train_02470</td>\n",
       "      <td>5反난WU선)y{비니$ 동맹Pkv의회 U탈리4 5거j압승</td>\n",
       "      <td>6</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>5반난WU선y비니 동맹Pkv의회 U탈리4 5거j압승</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>ynat-v1_train_00626</td>\n",
       "      <td>이란 반정(9위서 7행&amp;관{서 771j X;…주모자 h#</td>\n",
       "      <td>6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>이란 반정9위서 7행관서 771j X주모자 h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>ynat-v1_train_01117</td>\n",
       "      <td>무례 외:%고노 日e무* 교체ew부상4아베 11Ui.p</td>\n",
       "      <td>6</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>무례 외고노 일e무 교체ew부상4아베 11Uip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ynat-v1_train_00059</td>\n",
       "      <td>아사w 트K프 외교S험k없다고…i5오바I도]KR&lt;8였-</td>\n",
       "      <td>6</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>아사w 트K프 외교S험k없다고i5오바I도KR8였</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID                                text  target  \\\n",
       "639   ynat-v1_train_00639  中o선제적(입막음…2S·IBM|* 사이46안8 논의에 끌어들여       6   \n",
       "2470  ynat-v1_train_02470     5反난WU선)y{비니$ 동맹Pkv의회 U탈리4 5거j압승       6   \n",
       "626   ynat-v1_train_00626     이란 반정(9위서 7행&관{서 771j X;…주모자 h#       6   \n",
       "1117  ynat-v1_train_01117      무례 외:%고노 日e무* 교체ew부상4아베 11Ui.p       6   \n",
       "59    ynat-v1_train_00059      아사w 트K프 외교S험k없다고…i5오바I도]KR<8였-       6   \n",
       "\n",
       "     polluted_lv                       no_hanja  \n",
       "639     0.310345  중o선제적입막음2SIBM 사이46안8 논의에 끌어들여  \n",
       "2470    0.392857   5반난WU선y비니 동맹Pkv의회 U탈리4 5거j압승  \n",
       "626         0.32      이란 반정9위서 7행관서 771j X주모자 h  \n",
       "1117    0.346154     무례 외고노 일e무 교체ew부상4아베 11Uip  \n",
       "59      0.384615     아사w 트K프 외교S험k없다고i5오바I도KR8였  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = pd.read_csv('train-no_special-no_hanja-polluted_lv.csv').drop(columns=['Unnamed: 0'])\n",
    "train_dataset[(train_dataset['target']==6) & (0.30<train_dataset['polluted_lv']) & (train_dataset['polluted_lv']<0.40)].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA로 거지같은 텍스트 골라내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172a9e8fc6894811a6b607bc43ee2472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B' # 'beomi/Llama-3-Open-Ko-8B'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 클렌징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = '''주어진 데이터의 문맥이 어색한 부분을 교정하세요.\n",
    "1. 문맥이 어색하다면 교정하세요.\n",
    "2. 어색하지 않다면 그대로 출력하세요.\n",
    "3. 복원할 수 없다면 '복원 불가'라고 하세요.'''\n",
    "\n",
    "fewshot = [\n",
    "    {\"role\": \"user\", \"content\": \"올림픽|N데월드몰 0니j컬링y경R s벤0 진행\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"올림픽 롯데월드몰 미니 컬링 경품 이벤트 진행\"},\n",
    "    {\"role\": \"user\", \"content\": \"CJ헬로비전SKB 합병 무효 소송 잇따라…LGU＋ 법적 대응\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"CJ헬로비전SKB 합병 무효 소송 잇따라…LGU＋ 법적 대응\"},\n",
    "    {\"role\": \"user\", \"content\": \"정i 파1 미사z KT 이용기간 2e 단 Q분종U2보\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"복원 불가\"}\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "for data in train.iterrows():\n",
    "    data = data[1]\n",
    "    messages = [{\"role\": \"system\", \"content\": PROMPT}] + fewshot + [{\"role\": \"user\", \"content\": data['text']}]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    print(data['text'])\n",
    "    print(tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['스포츠', '사회', '문학', '영화', '문화', '언론', '경제', '정치', '기상', '문학'],\n",
       " ['스포츠', '스포츠', '스포츠', '스포츠', '스포츠', '스포츠', '스포츠', '스포츠', '스포츠', '스포츠'],\n",
       " ['정치', '정치', '정치', '정치', '정치', '정치', '정치', '정치', '정치', '외교'],\n",
       " ['사회', '교육', '법', '교육', '사회', '사회', '사회', '기술', '교육', '경제'],\n",
       " ['기술', '기술', '기술', '기술', '기술', '기술', '전자', '전자', '전자', '기술'],\n",
       " ['경제', '경제', '경제', '산업', '금융', '경제', '경제', '경제', '경제', '경제'],\n",
       " ['외교', '경제', '정치', '정치', '경제', '정치', '경제', '외교', '경제', '미국']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = '''당신은 기사 제목을 보고 어떤 분야의 기사인지 맞추는 전문가입니다.\n",
    "[지시사항]\n",
    "1. 주어진 데이터는 개행 기호(\\\\n)로 구분된 뉴스 기사 제목들입니다.\n",
    "2. 주어진 데이터에는 임의의 자리에 문맥에 맞지 않는 글자가 무작위로 삽입되어 있습니다.\n",
    "3. 해당 데이터들을 가장 잘 표현하는 포괄적인 기사 분야를 한 단어로만 출력하세요.'''\n",
    "\n",
    "fewshot = [\n",
    "    {\"role\": \"user\", \"content\": \"SW최Z 19도 날v 따뜻O져중부 밤늦게 m 조금\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"생활\"},\n",
    "    {\"role\": \"user\", \"content\": \"게Z판 통 2 편입생 모집 0카c톡bb진행\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"사회\"},\n",
    "    {\"role\": \"user\", \"content\": \"말로하E 대화형 내비 나왔다S8T Tnx구출시종\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"기술\"},\n",
    "    {\"role\": \"user\", \"content\": \"T치K2 Y루마니아 방문정 회R 함y 기도\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"해외\"}\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "keywords = []\n",
    "for i in range(7):\n",
    "    keys = []\n",
    "    for j in range(10):\n",
    "        data = train_dataset[(train_dataset['target']==i) & (0.30<train_dataset['polluted_lv']) & (train_dataset['polluted_lv']<0.40)].sample(5)['no_hanja'].values\n",
    "        data = '\\n'.join(data)\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": f\"{PROMPT}\"}] + fewshot + [{\"role\": \"user\", \"content\": f\"{data}\"}]\n",
    "\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(model.device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1024,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        keys.append(tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "\n",
    "    keywords.append(keys)\n",
    "\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['문학', '스포츠', '정치', '사회', '기술', '경제', '외교']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = '''제공된 데이터들은 뉴스 기사 분야들입니다. 전체 단어를 아우를 수 있는 하나의 키워드를 한 단어로 출력하세요.'''\n",
    "fewshot = [\n",
    "    {\"role\": \"user\", \"content\": \"외교, 정치, 경제, 외교, 경제, 정치, 외교, 외교, 외교, '정치\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"외교\"}\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "final_keys = []\n",
    "for keys in keywords:\n",
    "    keys = ', '.join(keys)\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": f\"{PROMPT}\"}] + fewshot + [{\"role\": \"user\", \"content\": f\"{keys}\"}]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    final_keys.append(tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "\n",
    "final_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "key_maps = {}\n",
    "for idx, key in enumerate(final_keys):\n",
    "    key_maps[idx] = key\n",
    "\n",
    "with open('./key_maps.json', 'w') as f:\n",
    "    json.dump(key_maps, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
