{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import hanja # !pip install hanja\n",
    "import pandas as pd\n",
    "\n",
    "train_dataset = pd.read_csv('../../../data/train.csv')\n",
    "train_texts = list(train_dataset['text'].values)\n",
    "IDs = list(train_dataset['ID'].values)\n",
    "\n",
    "korean = re.compile('[^가-힣...…\\s]+') # 한국어, ..., …, 공백 외의 문자가 한 번 이상 등장할 경우에 대한 패턴\n",
    "\n",
    "train_dataset['polluted_lv'] = None\n",
    "train_dataset['no_hanja'] = None\n",
    "for id_ in IDs:\n",
    "    text = train_dataset[train_dataset['ID']==id_]['text'].values[0]\n",
    "    text = hanja.translate(text, 'substitution')\n",
    "    text = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", text)\n",
    "    \n",
    "    results = korean.findall(text)\n",
    "    total = sum([len(r) for r in results])\n",
    "    prob = total / len(text)\n",
    "    train_dataset.loc[train_dataset['ID']==id_, 'polluted_lv'] = prob\n",
    "    train_dataset.loc[train_dataset['ID']==id_, 'no_hanja'] = text\n",
    "\n",
    "# train_dataset.to_csv('train-no_special-no_hanja-polluted_lv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>polluted_lv</th>\n",
       "      <th>no_hanja</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>ynat-v1_train_00132</td>\n",
       "      <td>올해iEIDFO*상에 멀리 개 짖는 소w가 들\"S</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>올해iEIDFO상에 멀리 개 짖는 소w가 들S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>ynat-v1_train_00717</td>\n",
       "      <td>기중기  \\트B들0 기#7*7]y 세계K최  X용</td>\n",
       "      <td>0</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>기중기  트B들0 기77y 세계K최  X용</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>ynat-v1_train_01240</td>\n",
       "      <td>날씨-투표% \"는 ~l비 T다 오fm터8G쳐JY일</td>\n",
       "      <td>0</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>날씨투표 는 l비 T다 오fm터8G쳐JY일</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2723</th>\n",
       "      <td>ynat-v1_train_02723</td>\n",
       "      <td>8간Z\"로벌 트렌드 n035·마르케Q의%서재에서</td>\n",
       "      <td>0</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>8간Z로벌 트렌드 n035마르케Q의서재에서</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>ynat-v1_train_00215</td>\n",
       "      <td>폭\"a!4 야o0 보러 x려든z시민xM+종*화I관6전시 인기</td>\n",
       "      <td>0</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>폭a4 야o0 보러 x려든z시민xM종화I관6전시 인기</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID                               text  target  \\\n",
       "132   ynat-v1_train_00132        올해iEIDFO*상에 멀리 개 짖는 소w가 들\"S       0   \n",
       "717   ynat-v1_train_00717        기중기  \\트B들0 기#7*7]y 세계K최  X용       0   \n",
       "1240  ynat-v1_train_01240        날씨-투표% \"는 ~l비 T다 오fm터8G쳐JY일       0   \n",
       "2723  ynat-v1_train_02723         8간Z\"로벌 트렌드 n035·마르케Q의%서재에서       0   \n",
       "215   ynat-v1_train_00215  폭\"a!4 야o0 보러 x려든z시민xM+종*화I관6전시 인기       0   \n",
       "\n",
       "     polluted_lv                       no_hanja  \n",
       "132         0.32      올해iEIDFO상에 멀리 개 짖는 소w가 들S  \n",
       "717     0.304348        기중기  트B들0 기77y 세계K최  X용  \n",
       "1240    0.347826        날씨투표 는 l비 T다 오fm터8G쳐JY일  \n",
       "2723    0.304348        8간Z로벌 트렌드 n035마르케Q의서재에서  \n",
       "215     0.344828  폭a4 야o0 보러 x려든z시민xM종화I관6전시 인기  "
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = pd.read_csv('train-no_special-no_hanja-polluted_lv.csv').drop(columns=['Unnamed: 0'])\n",
    "train_dataset[(train_dataset['target']==0) & (0.30<train_dataset['polluted_lv']) & (train_dataset['polluted_lv']<0.35)].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059de9ba33824bcea69b0e326bef9c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B' # 'beomi/Llama-3-Open-Ko-8B'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['사회', '예술', '경제', '신문', '사회', '문화', '사회', '스포츠', '문화', '자동차'],\n",
       " ['스포츠', '스포츠', '스포츠', '스포츠', '스포츠', '스포츠', '스포츠', '스포츠', '스포츠', '스포츠'],\n",
       " ['정치', '정치', '정치', '정치', '정치', '정치', 'techno', '경제', '정치', '정치'],\n",
       " ['교육', '경제', '정치', '경제', '교육', '기술', '정치', '경제', '교육', '정치'],\n",
       " ['IT/기술', '기술', '기술', '기술', '기술', ' 기술', ' 기술', '기술', '디지털', '기술'],\n",
       " ['경제', '기술', '기술', '경제', '경제', '경제', '경제', '경제', '경제', 'เทคโนโลย'],\n",
       " ['외교', '외교', '경제', '정치', '정치', '외교', '국제', '외교', '정치', '외교']]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = '''당신은 기사 제목을 보고 어떤 분야의 기사인지 맞추는 전문가입니다.\n",
    "[지시사항]\n",
    "1. 주어진 데이터는 개행 기호(\\\\n)로 구분된 뉴스 기사 제목들입니다.\n",
    "2. 주어진 데이터에는 임의의 자리에 문맥에 맞지 않는 글자가 무작위로 삽입되어 있습니다.\n",
    "3. 해당 데이터들을 가장 잘 표현하는 포괄적인 기사 분야를 한 단어로만 출력하세요.'''\n",
    "\n",
    "fewshot = [\n",
    "    {\"role\": \"user\", \"content\": \"신간2문학A음악이 q야기ji\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"문화\"},\n",
    "    {\"role\": \"user\", \"content\": \"정부 4월 한반도 위설d근거O다Y되J Q6야d합\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"정치\"},\n",
    "    {\"role\": \"user\", \"content\": \"충북교육청 노h조합mYfR동K군별 4양한 목소OH용\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"사회\"},\n",
    "    {\"role\": \"user\", \"content\": \"국제k호단체HrUTs 유엔 테단체 1정 움직f에k우려\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"해외\"}\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "keywords = []\n",
    "for i in range(7):\n",
    "    keys = []\n",
    "    for j in range(10):\n",
    "        data = train_dataset[(train_dataset['target']==i) & (0.30<train_dataset['polluted_lv']) & (train_dataset['polluted_lv']<0.40)].sample(5)['no_hanja'].values\n",
    "        data = '\\n'.join(data)\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": f\"{PROMPT}\"}] + fewshot + [{\"role\": \"user\", \"content\": f\"{data}\"}]\n",
    "\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(model.device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1024,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        keys.append(tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "\n",
    "    keywords.append(keys)\n",
    "\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사회', '스포츠', '정치', '교육, 정치, 경제', '디지털', '경제', '외교']"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = '''제공된 데이터들은 뉴스 기사 분야 10개에 대한 단어입니다. 키워드를 추출하세요.\n",
    "- 많이 등장하는 단어보다 전체 단어들의 맥락을 고려하세요.\n",
    "- 여러 개의 키워드가 있다면 가장 포괄적인 키워드 한 개만 출력하세요.'''\n",
    "\n",
    "# fewshot = [\n",
    "#     {\"role\": \"user\", \"content\": \"경제, 문화, 예술, 과학, 문화, 예술, 경제\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"생활\"}\n",
    "# ]\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "final_keys = []\n",
    "for keys in keywords:\n",
    "    keys = ', '.join(keys)\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": f\"{PROMPT}\"}] + fewshot + [{\"role\": \"user\", \"content\": f\"{keys}\"}]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    final_keys.append(tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "\n",
    "final_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "key_maps = {}\n",
    "for idx, key in enumerate(final_keys):\n",
    "    key_maps[idx] = key\n",
    "\n",
    "with open('./key_maps.json', 'w') as f:\n",
    "    json.dump(key_maps, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
