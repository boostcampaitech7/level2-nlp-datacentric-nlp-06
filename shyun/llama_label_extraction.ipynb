{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hanja'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhanja\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../../data/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hanja'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import hanja # !pip install hanja\n",
    "import pandas as pd\n",
    "\n",
    "train_dataset = pd.read_csv('../../../data/train.csv')\n",
    "train_texts = list(train_dataset['text'].values)\n",
    "IDs = list(train_dataset['ID'].values)\n",
    "\n",
    "korean = re.compile('[^가-힣...…\\s]+') # 한국어, ..., …, 공백 외의 문자가 한 번 이상 등장할 경우에 대한 패턴\n",
    "\n",
    "train_dataset['polluted_lv'] = None\n",
    "train_dataset['no_hanja'] = None\n",
    "for id_ in IDs:\n",
    "    text = train_dataset[train_dataset['ID']==id_]['text'].values[0]\n",
    "    text = hanja.translate(text, 'substitution')\n",
    "    text = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", text)\n",
    "    \n",
    "    results = korean.findall(text)\n",
    "    total = sum([len(r) for r in results])\n",
    "    prob = total / len(text)\n",
    "    train_dataset.loc[train_dataset['ID']==id_, 'polluted_lv'] = prob\n",
    "    train_dataset.loc[train_dataset['ID']==id_, 'no_hanja'] = text\n",
    "\n",
    "# train_dataset.to_csv('train-no_special-no_hanja-polluted_lv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train = pd.read_csv('train-no_special-no_hanja-polluted_lv.csv').drop(columns=['Unnamed: 0'])\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_dataset\u001b[49m[(train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m6\u001b[39m) \u001b[38;5;241m&\u001b[39m (\u001b[38;5;241m0.30\u001b[39m\u001b[38;5;241m<\u001b[39mtrain_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolluted_lv\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m&\u001b[39m (train_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolluted_lv\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m0.40\u001b[39m)]\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# train = pd.read_csv('train-no_special-no_hanja-polluted_lv.csv').drop(columns=['Unnamed: 0'])\n",
    "train_dataset[(train_dataset['target']==6) & (0.30<train_dataset['polluted_lv']) & (train_dataset['polluted_lv']<0.40)].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA로 거지같은 텍스트 골라내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172a9e8fc6894811a6b607bc43ee2472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B' # 'beomi/Llama-3-Open-Ko-8B'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 클렌징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = '''주어진 데이터의 문맥이 어색한 부분을 교정하세요.\n",
    "1. 문맥이 어색하다면 교정하세요.\n",
    "2. 어색하지 않다면 그대로 출력하세요.\n",
    "3. 복원할 수 없다면 '복원 불가'라고 하세요.'''\n",
    "\n",
    "fewshot = [\n",
    "    {\"role\": \"user\", \"content\": \"올림픽|N데월드몰 0니j컬링y경R s벤0 진행\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"올림픽 롯데월드몰 미니 컬링 경품 이벤트 진행\"},\n",
    "    {\"role\": \"user\", \"content\": \"CJ헬로비전SKB 합병 무효 소송 잇따라…LGU＋ 법적 대응\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"CJ헬로비전SKB 합병 무효 소송 잇따라…LGU＋ 법적 대응\"},\n",
    "    {\"role\": \"user\", \"content\": \"정i 파1 미사z KT 이용기간 2e 단 Q분종U2보\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"복원 불가\"}\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "for data in train.iterrows():\n",
    "    data = data[1]\n",
    "    messages = [{\"role\": \"system\", \"content\": PROMPT}] + fewshot + [{\"role\": \"user\", \"content\": data['text']}]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    print(data['text'])\n",
    "    print(tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['스포츠', '사회', '문학', '영화', '문화', '언론', '경제', '정치', '기상', '문학'],\n",
       " ['스포츠', '스포츠', '스포츠', '스포츠', '스포츠', '스포츠', '스포츠', '스포츠', '스포츠', '스포츠'],\n",
       " ['정치', '정치', '정치', '정치', '정치', '정치', '정치', '정치', '정치', '외교'],\n",
       " ['사회', '교육', '법', '교육', '사회', '사회', '사회', '기술', '교육', '경제'],\n",
       " ['기술', '기술', '기술', '기술', '기술', '기술', '전자', '전자', '전자', '기술'],\n",
       " ['경제', '경제', '경제', '산업', '금융', '경제', '경제', '경제', '경제', '경제'],\n",
       " ['외교', '경제', '정치', '정치', '경제', '정치', '경제', '외교', '경제', '미국']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = '''당신은 기사 제목을 보고 어떤 분야의 기사인지 맞추는 전문가입니다.\n",
    "[지시사항]\n",
    "1. 주어진 데이터는 개행 기호(\\\\n)로 구분된 뉴스 기사 제목들입니다.\n",
    "2. 주어진 데이터에는 임의의 자리에 문맥에 맞지 않는 글자가 무작위로 삽입되어 있습니다.\n",
    "3. 해당 데이터들을 가장 잘 표현하는 포괄적인 기사 분야를 한 단어로만 출력하세요.'''\n",
    "\n",
    "fewshot = [\n",
    "    {\"role\": \"user\", \"content\": \"SW최Z 19도 날v 따뜻O져중부 밤늦게 m 조금\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"생활\"},\n",
    "    {\"role\": \"user\", \"content\": \"게Z판 통 2 편입생 모집 0카c톡bb진행\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"사회\"},\n",
    "    {\"role\": \"user\", \"content\": \"말로하E 대화형 내비 나왔다S8T Tnx구출시종\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"기술\"},\n",
    "    {\"role\": \"user\", \"content\": \"T치K2 Y루마니아 방문정 회R 함y 기도\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"해외\"}\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "keywords = []\n",
    "for i in range(7):\n",
    "    keys = []\n",
    "    for j in range(10):\n",
    "        data = train[(train['target']==i) & (0.30<train['polluted_lv']) & (train['polluted_lv']<0.40)].sample(5)['no_hanja'].values\n",
    "        data = '\\n'.join(data)\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": f\"{PROMPT}\"}] + fewshot + [{\"role\": \"user\", \"content\": f\"{data}\"}]\n",
    "\n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(model.device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1024,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        keys.append(tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "\n",
    "    keywords.append(keys)\n",
    "\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['문학', '스포츠', '정치', '사회', '기술', '경제', '외교']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROMPT = '''제공된 데이터들은 뉴스 기사 분야들입니다. 전체 단어를 아우를 수 있는 하나의 키워드를 한 단어로 출력하세요.'''\n",
    "fewshot = [\n",
    "    {\"role\": \"user\", \"content\": \"외교, 정치, 경제, 외교, 경제, 정치, 외교, 외교, 외교, '정치\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"외교\"}\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "final_keys = []\n",
    "for keys in keywords:\n",
    "    keys = ', '.join(keys)\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": f\"{PROMPT}\"}] + fewshot + [{\"role\": \"user\", \"content\": f\"{keys}\"}]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    final_keys.append(tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True))\n",
    "\n",
    "final_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
