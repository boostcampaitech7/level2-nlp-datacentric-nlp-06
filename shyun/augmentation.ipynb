{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>total</th>\n",
       "      <th>train</th>\n",
       "      <th>valid</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>336</td>\n",
       "      <td>280</td>\n",
       "      <td>60</td>\n",
       "      <td>0.810345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "      <td>294</td>\n",
       "      <td>65</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>309</td>\n",
       "      <td>241</td>\n",
       "      <td>60</td>\n",
       "      <td>0.78125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>314</td>\n",
       "      <td>213</td>\n",
       "      <td>73</td>\n",
       "      <td>0.632184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>0.907692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>329</td>\n",
       "      <td>255</td>\n",
       "      <td>70</td>\n",
       "      <td>0.819444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>358</td>\n",
       "      <td>275</td>\n",
       "      <td>75</td>\n",
       "      <td>0.873418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target total train valid        f1\n",
       "0       0   336   280    60  0.810345\n",
       "1       1   365   294    65  0.941176\n",
       "2       2   309   241    60   0.78125\n",
       "3       3   314   213    73  0.632184\n",
       "4       4   322   256    64  0.907692\n",
       "5       5   329   255    70  0.819444\n",
       "6       6   358   275    75  0.873418"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "targets = pd.DataFrame(columns=['target', 'total', 'train', 'valid', 'f1'])\n",
    "targets['target'] = np.arange(7)\n",
    "\n",
    "# path = '../../v1.3.1_aug3'\n",
    "path = '../../v1.3.3'\n",
    "\n",
    "total = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "val_pred = pd.read_csv(os.path.join(path, 'valid_output.csv'))\n",
    "\n",
    "val_ids = list(val_pred['ID'].values)\n",
    "train = total[~total['ID'].str.contains('|'.join(val_ids))]\n",
    "val_answer = total[total['ID'].str.contains('|'.join(val_ids))]\n",
    "\n",
    "label = range(7)\n",
    "for l in label:\n",
    "    targets.loc[targets['target']==l, 'total'] = len(total[total['target']==l])\n",
    "    targets.loc[targets['target']==l, 'train'] = len(train[train['target']==l])\n",
    "    targets.loc[targets['target']==l, 'valid'] = len(val_pred[val_pred['target']==l])\n",
    "\n",
    "    val_correct = val_pred[(val_pred['target']==l) & val_pred['ID'].str.contains('|'.join((val_answer[val_answer['target']==l]['ID'].values)))]\n",
    "    recall = len(val_correct) / (len(val_answer[val_answer['target']==l]))\n",
    "    precision = len(val_correct) / (len(val_pred[val_pred['target']==l]))\n",
    "\n",
    "    targets.loc[targets['target']==l, 'f1'] = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA~♡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>polluted_lv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>ynat-v1_train_02154</td>\n",
       "      <td>I 안a상회의는 반공화국 핵소동 모의판 n난</td>\n",
       "      <td>2</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>ynat-v1_train_02186</td>\n",
       "      <td>우리카드 펠리페 영입시즌 시작 전 두 번째 외국인 교체</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>ynat-v1_train_00383</td>\n",
       "      <td>석촌동 고분군 발굴 조사 설명회</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>ynat-v1_train_02566</td>\n",
       "      <td>베스트셀러 세이t곰돌이 c N복C 일은 매일Q어 인기</td>\n",
       "      <td>0</td>\n",
       "      <td>0.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>ynat-v1_train_00360</td>\n",
       "      <td>폭증한 신용대출도 가상화폐 시장으로 갔나</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID                            text  target  polluted_lv\n",
       "1036  ynat-v1_train_02154        I 안a상회의는 반공화국 핵소동 모의판 n난       2     0.125000\n",
       "1057  ynat-v1_train_02186  우리카드 펠리페 영입시즌 시작 전 두 번째 외국인 교체       1     0.000000\n",
       "191   ynat-v1_train_00383               석촌동 고분군 발굴 조사 설명회       0     0.000000\n",
       "1246  ynat-v1_train_02566   베스트셀러 세이t곰돌이 c N복C 일은 매일Q어 인기       0     0.172414\n",
       "178   ynat-v1_train_00360          폭증한 신용대출도 가상화폐 시장으로 갔나       5     0.000000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('key_maps.json', 'r', encoding='utf-8') as f:\n",
    "    key_maps = json.load(f)\n",
    "\n",
    "train = pd.read_csv('../../v1.3.0/train.csv')\n",
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df88e4ffa0f04d3e9f5f845ca9b3cfed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B' # 'beomi/Llama-3-Open-Ko-8B'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0974f1b484484b93ac6fcca127d565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea988b7d1c874768b710faf6d480b2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "labeling:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "train_fewshot = pd.read_csv('../../v1.3.0/train.csv') # text drop, cleanlab, no aug\n",
    "\n",
    "generated = pd.DataFrame(columns=['ID', 'text', 'target', 'polluted_lv'])\n",
    "for i in tqdm(range(3, 4), total=1, position=0):\n",
    "    key = key_maps[str(i)]\n",
    "    new_aug = pd.DataFrame(columns=['ID', 'text', 'target', 'polluted_lv'])\n",
    "\n",
    "    PROMPT = f'''당신은 '{key}' 분야의 기사 제목을 작성하는 전문 작가입니다. 주어진 예시를 바탕으로 다양한 형태의 기사 제목을 작성해 주세요.'''\n",
    "\n",
    "    shots = train_fewshot[(train_fewshot['target']==i)].sample(10, random_state=42)['text'].values\n",
    "    fewshot = []\n",
    "    for shot in shots:\n",
    "        fewshot.append({\"role\": \"user\", \"content\": f\"'{key}' 분야에 해당하는 기사 제목을 한 개만 생성하세요.\"})\n",
    "        fewshot.append({\"role\": \"assistant\", \"content\": shot})\n",
    "    # fewshot = [{\"role\": \"assistant\", \"content\": \"\\n\".join(shots)}]\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    cnt = 0\n",
    "    for j in tqdm(range(100), desc='labeling', total=100, position=1, leave=False):\n",
    "        # messages = [{\"role\": \"system\", \"content\": PROMPT}] + fewshot + [{\"role\": \"user\", \"content\": f\"'{key}' 분야에 해당하는 기사 제목을 생성하세요.\"}]\n",
    "        messages = [{\"role\": \"system\", \"content\": PROMPT}] + fewshot + [{\"role\": \"user\", \"content\": f\"'{key}' 분야에 해당하는 기사 제목을 한 개만 생성하세요.\"}]\n",
    "        \n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(model.device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=512,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "        result = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "        new_aug.loc[j] = [f'augmented-{i}-{j}', result, i, 0]\n",
    "        # print(result)\n",
    "\n",
    "    generated = pd.concat([generated, new_aug])\n",
    "\n",
    "len(generated)\n",
    "generated.to_csv('../../v1.3.2/augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6bc2e12d8247359cf3d3752d7fde5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# label=3인 데이터만 2배로 재작성\n",
    "\n",
    "train_fewshot = pd.read_csv('../../v1.3.0/train.csv') # text drop, cleanlab, no aug\n",
    "train_fewshot = train_fewshot[(train_fewshot['ID'].str.contains('ynat')) & (train_fewshot['target']==3) & (train_fewshot['polluted_lv']<0.2)]['text'].values\n",
    "\n",
    "PROMPT = f'''당신은 '사회' 분야의 기사 제목을 재작성하는 전문가입니다. 제공된 기사 제목의 의미를 보존하며 재작성 하세요.'''\n",
    "\n",
    "fewshot = [\n",
    "    {'role': 'user', 'content': '보령소식 보령시 시간선택제 공무원 3명 모집'},\n",
    "    {'role': 'assistant', 'content': '보령시에서 새로운 시간제 공무원 모집'}\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "generated = pd.DataFrame(columns=['ID', 'text', 'target', 'polluted_lv'])\n",
    "for idx, data in tqdm(enumerate(train_fewshot), total=len(train_fewshot)):\n",
    "    messages = [{\"role\": \"system\", \"content\": PROMPT}] + fewshot + [{\"role\": \"user\", \"content\": data}]\n",
    "    \n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=128,\n",
    "        eos_token_id=terminators,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    result = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    generated.loc[idx] = [f'gen-3-{idx}', result, 3, 0]\n",
    "\n",
    "# generated.to_csv('../../v1.3.3/generated.csv', index=False)\n",
    "# v132 = pd.read_csv('../../v1.3.2/train.csv')\n",
    "# v133 = pd.concat([v132, generated])\n",
    "# v133.to_csv('../../v1.3.3/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator_1 = pipeline('translation', model='NHNDQ/nllb-finetuned-en2ko', device=0, src_lang='kor_Hang', tgt_lang='eng_Latn', max_length=512)\n",
    "translator_2 = pipeline('translation', model='NHNDQ/nllb-finetuned-en2ko', device=0, src_lang='eng_Latn', tgt_lang='kor_Hang', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "data = pd.read_csv('../../v1.3.0/train.csv')\n",
    "\n",
    "rtt = pd.DataFrame(columns=['ID', 'text', 'target', 'polluted_lv'])\n",
    "for i in tqdm(range(7), desc='rtt', total=7, position=0):\n",
    "    target = data[(data['target']==i) & (data['polluted_lv']<=0.10)]['text'].values\n",
    "\n",
    "    rtt_tmp = pd.DataFrame(columns=['ID', 'text', 'target', 'polluted_lv'])\n",
    "    for j, text in tqdm(enumerate(target), desc=f'{i}', total=len(target), position=1, leave=False):\n",
    "        # print(text)\n",
    "        output = translator_1(text, max_length=512ㄴ)\n",
    "        # print(output[0]['translation_text'])\n",
    "        output = translator_2(output[0]['translation_text'], max_length=512)\n",
    "        # print(output[0]['translation_text'])\n",
    "        # print()\n",
    "        rtt_tmp.loc[j] = [f'rtt-{i}-{j}', output[0]['translation_text'], i, 0]\n",
    "    rtt = pd.concat([rtt, rtt_tmp])\n",
    "print(len(rtt))\n",
    "rtt.to_csv('../../v1.3.2/rtt_2_50.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2267"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "v131 = pd.read_csv('../../v1.3.1_aug3/train.csv')\n",
    "rtt = pd.read_csv('../../v1.3.2/rtt_2.csv')\n",
    "rtt['target'].value_counts()\n",
    "\n",
    "rttt = pd.DataFrame(columns=['ID', 'text', 'target', 'polluted_lv'])\n",
    "for i in range(7):\n",
    "    if i==3:\n",
    "        rttt = pd.concat([rttt, rtt[rtt['target']==i]])    \n",
    "    else:\n",
    "        rttt = pd.concat([rttt, rtt[rtt['target']==i].sample(50)])\n",
    "len(rttt)\n",
    "\n",
    "v132 = pd.concat([v131, rttt])\n",
    "v132.to_csv('../../v1.3.2/train.csv', index=False)\n",
    "len(v132)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "- v1.3.2, label=3에 SR 시도해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mix-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
