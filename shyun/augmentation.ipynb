{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>total</th>\n",
       "      <th>train</th>\n",
       "      <th>valid</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>336</td>\n",
       "      <td>258</td>\n",
       "      <td>77</td>\n",
       "      <td>0.787097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>365</td>\n",
       "      <td>289</td>\n",
       "      <td>68</td>\n",
       "      <td>0.902778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>309</td>\n",
       "      <td>248</td>\n",
       "      <td>52</td>\n",
       "      <td>0.79646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>248</td>\n",
       "      <td>176</td>\n",
       "      <td>48</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>262</td>\n",
       "      <td>56</td>\n",
       "      <td>0.896552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>329</td>\n",
       "      <td>270</td>\n",
       "      <td>62</td>\n",
       "      <td>0.826446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>358</td>\n",
       "      <td>269</td>\n",
       "      <td>91</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target total train valid        f1\n",
       "0       0   336   258    77  0.787097\n",
       "1       1   365   289    68  0.902778\n",
       "2       2   309   248    52   0.79646\n",
       "3       3   248   176    48  0.666667\n",
       "4       4   322   262    56  0.896552\n",
       "5       5   329   270    62  0.826446\n",
       "6       6   358   269    91  0.888889"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "targets = pd.DataFrame(columns=['target', 'total', 'train', 'valid', 'f1'])\n",
    "targets['target'] = np.arange(7)\n",
    "\n",
    "# path = '../../v1.3.1_aug3'\n",
    "path = '../../v1.3.2'\n",
    "\n",
    "total = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "val_pred = pd.read_csv(os.path.join(path, 'valid_output.csv'))\n",
    "\n",
    "val_ids = list(val_pred['ID'].values)\n",
    "train = total[~total['ID'].str.contains('|'.join(val_ids))]\n",
    "val_answer = total[total['ID'].str.contains('|'.join(val_ids))]\n",
    "\n",
    "label = range(7)\n",
    "for l in label:\n",
    "    targets.loc[targets['target']==l, 'total'] = len(total[total['target']==l])\n",
    "    targets.loc[targets['target']==l, 'train'] = len(train[train['target']==l])\n",
    "    targets.loc[targets['target']==l, 'valid'] = len(val_pred[val_pred['target']==l])\n",
    "\n",
    "    val_correct = val_pred[(val_pred['target']==l) & val_pred['ID'].str.contains('|'.join((val_answer[val_answer['target']==l]['ID'].values)))]\n",
    "    recall = len(val_correct) / (len(val_answer[val_answer['target']==l]))\n",
    "    precision = len(val_correct) / (len(val_pred[val_pred['target']==l]))\n",
    "\n",
    "    targets.loc[targets['target']==l, 'f1'] = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaMA~♡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>polluted_lv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>ynat-v1_train_02785</td>\n",
       "      <td>민주화운동 보상받으면 국가배상 안 돼헌재와 반대 판결</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>ynat-v1_train_02444</td>\n",
       "      <td>가상화폐 인기에 악성코드 기승랜섬웨어 신고 37배</td>\n",
       "      <td>4</td>\n",
       "      <td>0.074074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ynat-v1_train_00073</td>\n",
       "      <td>대한항공 인천바르셀로나 노선 첫 취항</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>ynat-v1_train_00776</td>\n",
       "      <td>민주 평창올림픽 성공 개최 총력전색깔론 한국당 맹공</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>ynat-v1_train_02187</td>\n",
       "      <td>주말 N 여행 강원권 쫀득한 찰옥수수일편단심 무궁화</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID                           text  target  polluted_lv\n",
       "1329  ynat-v1_train_02785  민주화운동 보상받으면 국가배상 안 돼헌재와 반대 판결       2     0.000000\n",
       "1172  ynat-v1_train_02444    가상화폐 인기에 악성코드 기승랜섬웨어 신고 37배       4     0.074074\n",
       "30    ynat-v1_train_00073           대한항공 인천바르셀로나 노선 첫 취항       5     0.000000\n",
       "368   ynat-v1_train_00776   민주 평창올림픽 성공 개최 총력전색깔론 한국당 맹공       2     0.000000\n",
       "1058  ynat-v1_train_02187   주말 N 여행 강원권 쫀득한 찰옥수수일편단심 무궁화       0     0.035714"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('key_maps.json', 'r', encoding='utf-8') as f:\n",
    "    key_maps = json.load(f)\n",
    "\n",
    "train = pd.read_csv('../../v1.3.0/train.csv')\n",
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12a09ea147d4a78bc1e23921f4f31b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B' # 'beomi/Llama-3-Open-Ko-8B'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0974f1b484484b93ac6fcca127d565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea988b7d1c874768b710faf6d480b2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "labeling:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "train_fewshot = pd.read_csv('../../v1.3.0/train.csv') # text drop, cleanlab, no aug\n",
    "\n",
    "generated = pd.DataFrame(columns=['ID', 'text', 'target', 'polluted_lv'])\n",
    "for i in tqdm(range(3, 4), total=1, position=0):\n",
    "    key = key_maps[str(i)]\n",
    "    new_aug = pd.DataFrame(columns=['ID', 'text', 'target', 'polluted_lv'])\n",
    "\n",
    "    PROMPT = f'''당신은 '{key}' 분야의 기사 제목을 작성하는 전문 작가입니다. 주어진 예시를 바탕으로 다양한 형태의 기사 제목을 작성해 주세요.'''\n",
    "\n",
    "    shots = train_fewshot[(train_fewshot['target']==i)].sample(10, random_state=42)['text'].values\n",
    "    fewshot = []\n",
    "    for shot in shots:\n",
    "        fewshot.append({\"role\": \"user\", \"content\": f\"'{key}' 분야에 해당하는 기사 제목을 한 개만 생성하세요.\"})\n",
    "        fewshot.append({\"role\": \"assistant\", \"content\": shot})\n",
    "    # fewshot = [{\"role\": \"assistant\", \"content\": \"\\n\".join(shots)}]\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    cnt = 0\n",
    "    for j in tqdm(range(100), desc='labeling', total=100, position=1, leave=False):\n",
    "        # messages = [{\"role\": \"system\", \"content\": PROMPT}] + fewshot + [{\"role\": \"user\", \"content\": f\"'{key}' 분야에 해당하는 기사 제목을 생성하세요.\"}]\n",
    "        messages = [{\"role\": \"system\", \"content\": PROMPT}] + fewshot + [{\"role\": \"user\", \"content\": f\"'{key}' 분야에 해당하는 기사 제목을 한 개만 생성하세요.\"}]\n",
    "        \n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(model.device)\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=512,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "        result = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "        new_aug.loc[j] = [f'augmented-{i}-{j}', result, i, 0]\n",
    "        # print(result)\n",
    "\n",
    "    generated = pd.concat([generated, new_aug])\n",
    "\n",
    "len(generated)\n",
    "generated.to_csv('../../v1.3.2/augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator_1 = pipeline('translation', model='NHNDQ/nllb-finetuned-en2ko', device=0, src_lang='kor_Hang', tgt_lang='eng_Latn', max_length=512)\n",
    "translator_2 = pipeline('translation', model='NHNDQ/nllb-finetuned-en2ko', device=0, src_lang='eng_Latn', tgt_lang='kor_Hang', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "data = pd.read_csv('../../v1.3.0/train.csv')\n",
    "\n",
    "rtt = pd.DataFrame(columns=['ID', 'text', 'target', 'polluted_lv'])\n",
    "for i in tqdm(range(7), desc='rtt', total=7, position=0):\n",
    "    target = data[(data['target']==i) & (data['polluted_lv']<=0.10)]['text'].values\n",
    "\n",
    "    rtt_tmp = pd.DataFrame(columns=['ID', 'text', 'target', 'polluted_lv'])\n",
    "    for j, text in tqdm(enumerate(target), desc=f'{i}', total=len(target), position=1, leave=False):\n",
    "        # print(text)\n",
    "        output = translator_1(text, max_length=512ㄴ)\n",
    "        # print(output[0]['translation_text'])\n",
    "        output = translator_2(output[0]['translation_text'], max_length=512)\n",
    "        # print(output[0]['translation_text'])\n",
    "        # print()\n",
    "        rtt_tmp.loc[j] = [f'rtt-{i}-{j}', output[0]['translation_text'], i, 0]\n",
    "    rtt = pd.concat([rtt, rtt_tmp])\n",
    "print(len(rtt))\n",
    "rtt.to_csv('../../v1.3.2/rtt_2_50.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2267"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "v131 = pd.read_csv('../../v1.3.1_aug3/train.csv')\n",
    "rtt = pd.read_csv('../../v1.3.2/rtt_2.csv')\n",
    "rtt['target'].value_counts()\n",
    "\n",
    "rttt = pd.DataFrame(columns=['ID', 'text', 'target', 'polluted_lv'])\n",
    "for i in range(7):\n",
    "    if i==3:\n",
    "        rttt = pd.concat([rttt, rtt[rtt['target']==i]])    \n",
    "    else:\n",
    "        rttt = pd.concat([rttt, rtt[rtt['target']==i].sample(50)])\n",
    "len(rttt)\n",
    "\n",
    "v132 = pd.concat([v131, rttt])\n",
    "v132.to_csv('../../v1.3.2/train.csv', index=False)\n",
    "len(v132)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "- v1.3.2, label=3에 SR 시도해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mix-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
